{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, optimizers, datasets, Sequential\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "tf.random.set_seed(2345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(50000, 32, 32, 3) (50000, 1) (10000, 32, 32, 3) (10000, 1)\n",
      "sample: (16, 32, 32, 3) (16, 1) tf.Tensor(0.0, shape=(), dtype=float32) tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "0 0 loss: 4.604842662811279\n",
      "0 100 loss: 4.600252151489258\n",
      "0 200 loss: 4.606687545776367\n",
      "0 300 loss: 4.606091022491455\n",
      "0 400 loss: 4.60454797744751\n",
      "0 500 loss: 4.550275802612305\n",
      "0 600 loss: 4.569247722625732\n",
      "0 700 loss: 4.311275959014893\n",
      "0 800 loss: 4.601819038391113\n",
      "0 900 loss: 4.5080790519714355\n",
      "0 1000 loss: 4.404435634613037\n",
      "0 1100 loss: 4.071237564086914\n",
      "0 1200 loss: 4.418947219848633\n",
      "0 1300 loss: 4.329734802246094\n",
      "0 1400 loss: 4.244333267211914\n",
      "0 1500 loss: 4.130109786987305\n",
      "0 1600 loss: 3.867675304412842\n",
      "0 1700 loss: 4.537472724914551\n",
      "0 1800 loss: 4.252060890197754\n",
      "0 1900 loss: 4.3944597244262695\n",
      "0 2000 loss: 4.253200531005859\n",
      "0 2100 loss: 3.8479163646698\n",
      "0 2200 loss: 4.462834358215332\n",
      "0 2300 loss: 4.104136943817139\n",
      "0 2400 loss: 4.178561210632324\n",
      "0 2500 loss: 3.9681029319763184\n",
      "0 2600 loss: 3.9120941162109375\n",
      "0 2700 loss: 3.926862955093384\n",
      "0 2800 loss: 3.57889461517334\n",
      "0 2900 loss: 4.162026405334473\n",
      "0 3000 loss: 3.709106922149658\n",
      "0 3100 loss: 3.837980270385742\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'correct' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-34512b24451d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'correct' is not defined"
     ]
    }
   ],
   "source": [
    "# 5 units of conv + max pooling\n",
    "# 卷积层\n",
    "conv_layers = [\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"), \n",
    "\n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"), \n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"), \n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"), \n",
    "\n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding=\"same\", activation=tf.nn.relu),\n",
    "    layers.MaxPool2D(pool_size=[2, 2], strides=2, padding=\"same\"), \n",
    "]\n",
    "\n",
    "def preprocess(x, y):\n",
    "    # [0 ~ 1]\n",
    "    x = tf.cast(x, dtype=tf.float32) / 255.\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y\n",
    "\n",
    "(x, y), (x_test, y_test) = datasets.cifar100.load_data()\n",
    "print(x.shape, y.shape, x_test.shape, y_test.shape)\n",
    "\n",
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.shuffle(10000).map(preprocess).batch(64)\n",
    "\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_db = test_db.map(preprocess).batch(64)\n",
    "\n",
    "sample = next(iter(train_db))\n",
    "print('sample:', sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))\n",
    "\n",
    "optimizer = optimizers.Adam(lr=1e-4)\n",
    "\n",
    "# [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "conv_net = Sequential(conv_layers)\n",
    "\n",
    "# 全链接层\n",
    "fc_net = Sequential([\n",
    "    layers.Dense(256, activation=tf.nn.relu),\n",
    "    layers.Dense(128, activation=tf.nn.relu),\n",
    "    layers.Dense(100, activation=None)\n",
    "])\n",
    "\n",
    "conv_net.build(input_shape=[None, 32, 32, 3])\n",
    "fc_net.build(input_shape=[None, 512])\n",
    "\n",
    "# [1, 2] + [3, 4] => [1, 2, 3, 4] \n",
    "variables = conv_net.trainable_variables + fc_net.trainable_variables\n",
    "\n",
    "for epoch in range(50):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 32, 32, 3] => [b, 1, 1, 512]\n",
    "            out = conv_net(x)\n",
    "            # flatten, => [b, 512]\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            # [b, 512] => [b, 100]\n",
    "            logits = fc_net(out)\n",
    "            # [b] => [b, 100]\n",
    "            y_onehot = tf.one_hot(y, depth=100)\n",
    "            # compute loss\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "        \n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss:', float(loss))\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "    for x, y in test_db:\n",
    "        out = conv_net(x)\n",
    "        out = tf.reshape(out, [-1, 512])\n",
    "        logits = fc_net(out)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "        correct = tf.cast(tf.equal(correct, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc:', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}