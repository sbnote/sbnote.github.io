{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 均方差误差函数\n",
    "均方差（Mean Squared Error，简称MSE），把输出向量和真实向量映射到笛卡尔坐标系的两个点上，通过计算这两个点之间的欧式距离（准确地说是欧式距离的平方）来衡量两个向量之间的差距：\n",
    "\n",
    "\n",
    "$$\\displaystyle MSE(y,o)\\stackrel{\\mathrm{\\Delta}}{=}\\frac{1}{d_{out}}\\sum^{d^{out}}_{i=1}(y_i-o_i)$$\n",
    "\n",
    "当MSE函数达到最小值0时，输出等于真实标签，此时神经网络的参数达到最优状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(\n[[ 2.1270916   1.692834   -0.9886939   0.46138158  0.35399404 -0.7563028\n  -1.2045842  -2.8295927   1.5409844  -1.0450587 ]\n [ 0.91720855 -0.16368659 -0.25263077 -1.1987484   0.07164879 -0.6429402\n   0.8043688  -1.1519246  -0.62885076  0.04400402]], shape=(2, 10), dtype=float32)\ntf.Tensor(\n[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]], shape=(2, 10), dtype=float32)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.9816631 , 0.85562164], dtype=float32)>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "# 构造网络输出\n",
    "o = tf.random.normal([2, 10]) \n",
    "print(o)\n",
    "\n",
    "# 构造真实值\n",
    "y_onehot = tf.constant([1,3])  \n",
    "y_onehot = tf.one_hot(y_onehot, depth=10)\n",
    "print(y_onehot)\n",
    "\n",
    "# 计算每个样本的均方差\n",
    "loss = keras.losses.MSE(y_onehot, o) \n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=1.4186424>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "# 计算平均样本的均方差\n",
    "loss = tf.reduce_mean(loss)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=1.4186424>"
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# 通过层方式计算均方差（调用__call_函数即可完成前向计算）\n",
    "criteon = keras.losses.MeanSquaredError()\n",
    "loss = criteon(y_onehot, o)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉熵误差函数\n",
    "\n",
    "#### 熵(Entropy)\n",
    "熵越大，代表不确定性越大，信息量也就越大。某个 分布𝑃(𝑖)的熵定义为\n",
    "\n",
    "$$H(P)\\stackrel{\\mathrm{\\Delta}}{=}-\\sum_{i=1}𝑃(i)log_2𝑃(i)$$\n",
    "举个例子，对于 4 分类问题，如果某个 样本的真实标签是第 4 类，那么标签的 One-hot 编码为[0,0,0,1]，即这张图片的分类是唯一 确定的，它属于第 4 类的概率𝑃(𝑦为 4|𝒙) = 1，不确定性为 0，它的熵可以简单的计算为:\n",
    "$$−0 ∙ log2 0 − 0 ∙ log2 0 − 0 ∙ log2 0 − 1 ∙ log2 1 = 0$$\n",
    "也就是说，对于确定的分布，熵为 0，不确定性最低。 如果它预测的概率分布是[0.1,0.1,0.1,0.7]，它的熵可以计算为:\n",
    "$$−0.1 ∙ log2 0.1 − 0.1 ∙ log2 0.1 − 0.1 ∙ log2 0.1 − 0.7 ∙ log2 0.7 ≈ 1.356$$\n",
    "这种情况比前面确定性类别的例子的确定性要稍微大点。\n",
    "\n",
    "考虑随机分类器，它每个类别的预测概率是均等的:[0.25,0.25,0.25,0.25]，同样的方\n",
    "法，可以计算它的熵约为 2，这种情况的不确定性略大于上面一种情况。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor([0.25 0.25 0.25 0.25], shape=(4,), dtype=float32)\ntf.Tensor([-0.5 -0.5 -0.5 -0.5], shape=(4,), dtype=float32)\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.0>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 计算随机分类器的熵（每个类别预测概率均等）\n",
    "a = tf.fill([4], 0.25) \n",
    "print(a)\n",
    "print(a*tf.math.log(a)/tf.math.log(2.))\n",
    "-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "tf.Tensor(1.3567797, shape=(), dtype=float32)\ntf.Tensor(0.24194068, shape=(), dtype=float32)\n"
    }
   ],
   "source": [
    "# 计算概率分布为0.1,0.1,0.1,0.7的熵\n",
    "a = tf.constant([0.1, 0.1, 0.1, 0.7])\n",
    "print(-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.)))\n",
    "\n",
    "# 计算概率分布为0.01, 0.01, 0.01, 0.97的熵\n",
    "a = tf.constant([0.01, 0.01, 0.01, 0.97])\n",
    "print(-tf.reduce_sum(a*tf.math.log(a)/tf.math.log(2.)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 交叉熵(Cross Entropy)\n",
    "$$H(p||q)\\stackrel{\\mathrm{\\Delta}}{=}-\\sum{p(i)log_2q(i)}$$\n",
    "通过变换，交叉熵可以分解为p的熵H(p)和p与q的KL散度(Kullback-Leibler Divergence)的和：\n",
    "$$H(p||q)=H(p)+D_{KL}(p||q)$$\n",
    "其中KL定义为：\n",
    "$$D_{KL}(p||q)=\\sum{p(i)log\\Big(\\frac{p(i)}{q(i)}\\Big)}$$\n",
    "KL 散度是 Solomon Kullback 和 Richard A. Leibler 在 1951 年提出的用于衡量 2 个分布之间 距离的指标。𝑝 = 𝑞时，$𝐷_{𝐾𝐿}(𝑝||𝑞)$取得最小值0，𝑝与𝑞之间的差距越大，$𝐷_{𝐾𝐿}(𝑝||𝑞)$也越 大。\n",
    "\n",
    "交叉熵可以很好地衡量2个分布之间的“距离”。特别地，当分类总是中y的编码分布采用One-hot编码y时：H(p) = 0，此时\n",
    "$$H(p||q) = H(p) + D_{KL}(p||q) = D_{KL}(p||q)$$\n",
    "退化到真实标签分布y与输出概率分布o之间的KL散度上。\n",
    "\n",
    "根据KL散度的定义，我们推导分类总是中交叉熵的计算表达式：\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(p||q) = D_{p||q} &= \\sum_j{y_ilog\\left(\\frac{y_j}{o_j}\\right)} \\\\\n",
    "&=1·log\\frac{1}{o_i} + \\sum_{j \\neq i}0·log\\left(\\frac{0}{o_j}\\right) \\\\\n",
    "&= -logo_i\n",
    "\\end{align*}\n",
    "$$\n",
    "其中i为One-hot编码中为1的索引号，也是当前输入的真实类别。\n",
    "\n",
    "可以看到，L只与真实 类别𝑖上的概率$𝑜_𝑖$有关，对应概率$𝑜_𝑖$越大，𝐻(𝑝||𝑞)越小。当对应类别上的概率为 1 时，交叉 熵𝐻(𝑝||𝑞)取得最小值 0，此时网络输出𝒐与真实标签𝒚完全一致，神经网络取得最优状态。\n",
    "\n",
    "因此最小化交叉熵损失函数的过程也是最大化正确类别的预测概率的过程。从这个角 度去理解交叉熵损失函数，非常地直观易懂。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "举例说明加深理解：Classification问题\n",
    "\n",
    "$P_1 = [1\\quad 0\\quad 0\\quad 0\\quad 0]$, 真实值，5张图片，第1张是狗，其他的都不是。\n",
    "\n",
    "**预测一：**\n",
    "\n",
    "$Q_1 = [0.4\\quad 0.3\\quad 0.05\\quad 0.05\\quad 0.2]$，预测概率\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(P_1, Q_1) &= -\\sum_iP_1(i)logQ_1(i) \\\\\n",
    "&= -(1log0.4 + 0log0.3 + 0log0.05 + 0log0.05 + 0log0.2) \\\\\n",
    "&= -log0.4 \\\\\n",
    "&\\approx 0.916\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "交叉熵越大，预测越不准确\n",
    "\n",
    "**预测二：** \n",
    "\n",
    "$Q_1 = [0.98\\quad 0.01\\quad 0\\quad 0\\quad 0.01]$，预测概率\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "H(P_1, Q_1) &= -\\sum_iP_1(i)logQ_1(i) \\\\\n",
    "&= -(1log0.98 + 0log0.01 + 0log0 + 0log0 + 0log0.01) \\\\\n",
    "&= -log0.98 \\\\\n",
    "&\\approx 0.02\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "交叉熵越小，预测越准确\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=1.3862944>"
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "\n",
    "tf.losses.categorical_crossentropy([0,1,0,0], [0.25, 0.25, 0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.3978953>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "tf.losses.categorical_crossentropy([0,1,0,0], [0.1, 0.1, 0.8, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.030459179>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "tf.losses.categorical_crossentropy([0,1,0,0], [0.01, 0.97, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.00029999955>"
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "criteon([0,1,0,0], [0.01, 0.97, 0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.3025842>"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "tf.losses.BinaryCrossentropy()([1], [0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=2.3025842>"
     },
     "metadata": {},
     "execution_count": 39
    }
   ],
   "source": [
    "tf.losses.binary_crossentropy([1], [0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}